\documentclass[a4paper]{article}
\setlength{\topmargin}{-1.0in}
\setlength{\oddsidemargin}{-0.2in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{10.5in}
\setlength{\textwidth}{6.5in}
\setlength{\parindent}{0pt}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{minted}
\usepackage[dvipsnames]{xcolor}
\usepackage{mathpartir}
\newlist{sollist}{itemize}{1}
\setlist[sollist]{label=$\implies$}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator{\Tr}{Tr}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Assignment 1},
    pdfpagemode=FullScreen,
    }
\def\endproofmark{$\Box$}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newenvironment{proof}{\par{\bf Proof}:}{\endproofmark\smallskip}
\begin{document}
\begin{center}
{\large \bf \color{red}  Department of Computer Science} \\
{\large \bf \color{red}  Ashoka University} \\

\vspace{0.1in}

{\large \bf \color{blue} Introduction to Machine Learning: CS-3410-1}

\vspace{0.05in}

    { \bf \color{YellowOrange} Assignment 1}
\end{center}
\medskip

{\textbf{Name: Dhruman Gupta} }

\bigskip
\hrule

\section{Poisson Distribution as an Exponential Family}
\subsection{Finding the parameters of the exponential family}
Each member of the univariate exponential family has the form:
\begin{equation*}
    p(x | \eta) = b(x) \exp(\eta x - A(\eta))
\end{equation*}

We want to find these parameters for the poisson distribution, which is given by:
\begin{equation*}
    p(x | \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
\end{equation*}

For the case of the poisson distribution, we have:
\begin{align*}
    \eta &= \log \lambda \\
    b(x) &= \frac{1}{x!} \\
    A(\eta) &= e^\eta
\end{align*}

Plugging in the values, we get:
\begin{align*}
    p(x | \eta) &= b(x) \exp(\eta x - A(\eta)) \\
    &= \frac{1}{x!} \exp((\log \lambda) x - e^{\log \lambda}) \\
    &= \frac{1}{x!} \exp(- \lambda) \lambda^x \\
    &= \frac{\lambda^x e^{-\lambda}}{x!}
\end{align*}

This proves that the poisson distribution is a member of the univariate exponential family, and the exact parameters are those given above.

\subsection{Verifying $\mathbb{E}[X]$ and $Var(X)$}

\begin{enumerate}[label=(\alph*)]
    \item $\mathbb{E}[X] = A'(\eta)$. Let's first find $\mathbb{E}[X]$.
    \begin{align*}
        \mathbb{E}[X] &= \sum_{x=0}^{\infty} x p(x) \\
        &= \sum_{x=0}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!} = \sum_{x=1}^{\infty} x \frac{\lambda^x e^{-\lambda}}{x!} \\
        &= e^{-\lambda} \lambda \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!} \\
        &= e^{-\lambda} \lambda \sum_{y=0}^{\infty} \frac{\lambda^y}{y!} \\
        &= e^{-\lambda} \lambda e^{\lambda} \\
        &= \lambda
    \end{align*}

    This proves that $\mathbb{E}[X] = \lambda$. Now, $A(\eta) = e^\eta$. So, $A'(\eta) = e^\eta$. As $\eta = \log \lambda$, we have $A'(\eta) = \lambda$.

    This proves that $\mathbb{E}[X] = A'(\eta)$.

    \newpage
    
    \item $Var(X) = A''(\eta)$. Let's first find $Var(X)$. We will find $\mathbb{E}[X^2]$ first.
    \begin{align*}
        \mathbb{E}[X^2] &= \sum_{x=0}^{\infty} x^2 p(x) \\
        &= \sum_{x=0}^{\infty} x^2 \frac{\lambda^x e^{-\lambda}}{x!} = \sum_{x=1}^{\infty} x^2 \frac{\lambda^x e^{-\lambda}}{x!} \\
        &= e^{-\lambda} \lambda \sum_{x=1}^{\infty} x \frac{\lambda^{x-1}}{(x-1)!} \\
        &= e^{-\lambda} \lambda \left(
            \sum_{x=1}^{\infty} \frac{(x-1)\lambda^{x-1}}{(x-1)!} + \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1)!}
        \right)\\
        &= e^{-\lambda} \lambda \left(
            e^{\lambda} + \lambda \sum_{y=0}^{\infty} \frac{\lambda^y}{y!}
        \right)\\
        &= e^{-\lambda} \lambda \left(
            e^{\lambda} + \lambda e^{\lambda}
        \right)\\
        &= \lambda^2 + \lambda
    \end{align*}

    Now, $Var(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$.
    
    $A''(\eta) = \frac{d}{d\eta} A'(\eta) = e^\eta$. As $\eta = \log \lambda$, we have $A''(\eta) = \lambda$.\\

    This proves that $Var(X) = A''(\eta)$.    
\end{enumerate}

\newpage

\section{Linear Combination of Random Variables}
\subsection{Showing mean of Z}

\begin{align*}
    \mathbb{E}[Z] &= \mathbb{E}[\sum_{i=1}^{N} \omega_i X_i] \\
    &= \sum_{i=1}^{N} \mathbb{E}[\omega_i X_i] = \sum_{i=1}^{N} \omega_i \mathbb{E}[X_i] \\
    &= [\omega_1, \omega_2, \dots, \omega_N] \begin{bmatrix}
        \mathbb{E}[X_1] \\
        \mathbb{E}[X_2] \\
        \vdots \\
        \mathbb{E}[X_N]
      \end{bmatrix}\\
    &= \omega^T \mu
\end{align*}

\subsection{Showing variance of Z}

\begin{align*}
              Var(Z) &= Var(\sum_{i=1}^{N} \omega_i X_i) \\
    &= \sum_{i=1}^{N} \sum_{j=1}^{N} \omega_i \omega_j Cov(X_i, X_j) \\
    &= \sum_{i=1}^{N} \sum_{j=1}^{N} \omega_i \omega_j \Sigma_{ij} \\
    &= \omega^T \Sigma \omega
\end{align*}

\subsection{Bounding the correlation coefficient}

$\rho_{X_1, X_2} = \frac{Cov(X_1, X_2)}{\sigma_{X_1} \sigma_{X_2}}$, where \(\sigma_{X_i} = \sqrt{\operatorname{Var}(X_i)}\) for \(i=1,2\).\\

To use the Cauchy-Schwarz inequality, we first consider the 0-mean random variables:
\[
Y_1 = X_1 - \mathbb{E}[X_1] \quad \text{and} \quad Y_2 = X_2 - \mathbb{E}[X_2].
\]
Then, by definition, we have
\[
\operatorname{Cov}(X_1, X_2) = \mathbb{E}[Y_1 Y_2] \quad \text{and} \quad \sigma_{X_i}^2 = \mathbb{E}[Y_i^2] \text{ for } i=1,2.
\]

We want to show that $-1 \leq \rho_{X_1, X_2} \leq 1$, using the Cauchy-Schwarz inequality.\\

The Cauchy-Schwarz inequality states that:
\begin{equation*}
    |\mathbb{E}[X_1 X_2]| \leq \sqrt{\mathbb{E}[X_1^2] \mathbb{E}[X_2^2]}
\end{equation*}

Substituting the definitions, we obtain
\[
|\operatorname{Cov}(X_1, X_2)| \leq \sigma_{X_1}\sigma_{X_2}.
\]
Dividing both sides of the inequality by \(\sigma_{X_1}\sigma_{X_2}\) (which are positive), we have
\[
\left|\frac{\operatorname{Cov}(X_1, X_2)}{\sigma_{X_1}\sigma_{X_2}}\right| \leq 1.
\]
Thus,
\[
|\rho_{X_1, X_2}| \leq 1,
\]
which implies
\[
-1 \leq \rho_{X_1, X_2} \leq 1.
\]
This completes the proof.
\newpage

\subsection{Finding the expected value of a Gaussian}

We are asked to derive the value of the integral:
\begin{equation*}
    \int_{-\infty}^{\infty} \frac{x}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx
\end{equation*}

We will first use the substitution $u = \frac{x - \mu}{\sigma}$. Then, $dx = \sigma du$. Substituting, we get:
\begin{align*}
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma u + \mu) \exp(-\frac{(\sigma u + \mu - \mu)^2}{2\sigma^2}) du\\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} (\sigma u + \mu) \exp(-\frac{u^2}{2}) du\\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \mu \exp(-\frac{u^2}{2}) du + \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \sigma u \exp(-\frac{u^2}{2}) du
\end{align*}

Now, we have to evaluate the two integrals. Let's start with the first one:
\begin{align*}
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \mu \exp(-\frac{u^2}{2}) du\\
    &= \frac{\mu}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp(-\frac{u^2}{2}) du\\
    &= \frac{\mu}{\sqrt{2\pi}} \sqrt{2\pi}\\
    &= \mu
\end{align*}

Now, let's evaluate the second integral:
\begin{align*}
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \sigma u \exp(-\frac{u^2}{2}) du\\
    &= \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u \exp(-\frac{u^2}{2}) du
\end{align*}
Now, let $f(u) = u \exp(-\frac{u^2}{2})$. Note that $f(-u) = -u \exp(-\frac{u^2}{2}) = -f(u)$. So, $f$ is an odd function, i.e $f(-u) = -f(u)$. So, we can divide our integral into two parts:
\begin{align*}
    &= \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^{\infty} u \exp(-\frac{u^2}{2}) du\\
    &= \frac{\sigma}{\sqrt{2\pi}} \left( \int_{-\infty}^{0} u \exp(-\frac{u^2}{2}) du + \int_{0}^{\infty} u \exp(-\frac{u^2}{2}) du \right)\\
    &= \frac{\sigma}{\sqrt{2\pi}} \left( \int_{0}^{\infty} u \exp(-\frac{u^2}{2}) du - \int_{0}^{\infty} u \exp(-\frac{u^2}{2}) du \right)\\
    &= 0
\end{align*}

So, the final integral evaluates to $\mu$.

\newpage

\section{Multi-dimensional Output Model}

\subsection{Finding the closed form of the loss function}

We are given that:
\begin{equation*}
    J(\Theta) = \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{p} ((\Theta^T x^{(i)})_j - y_i^{(j)})^2
\end{equation*}

Let's first write out input data $X$ as a matrix:
\begin{equation*}
    X = \begin{bmatrix}
        (x^{(1)})^T \\
        (x^{(2)})^T \\
        \vdots \\
        (x^{(m)})^T
    \end{bmatrix}
\end{equation*}

Let's also write out the output data $Y$ as a matrix:
\begin{equation*}
    Y = \begin{bmatrix}
        (y^{(1)})^T \\
        (y^{(2)})^T \\
        \vdots \\
        (y^{(m)})^T
    \end{bmatrix}
\end{equation*}

Now, we can say that $\hat{Y} = X\Theta$. Our error is $Y - \hat{Y}$. Since, our loss function is essentially each input data's each component error squared, we can say that:
\begin{equation*}
    J(\Theta) = \frac{1}{2} \norm{Y - X\Theta}^2_F
\end{equation*}

Where $\norm{\cdot}_F$ is the Frobenius norm (this is just the square root of the sum of the squares of the elements of the matrix).

We can rewrite this as:
\begin{equation*}
    J(\Theta) = \frac{1}{2} \Tr((Y - X\Theta)^T (Y - X\Theta))
\end{equation*}

Since that is equivalent to the Frobenius norm. This gives us the closed form of the loss function.

\subsection{Finding the closed form solution for the loss function}

We want to minimise the loss function. So we need to take the gradient. For that, we will first expand the loss function and then take the gradient.

\begin{align*}
    J(\Theta) &= \frac{1}{2} \Tr((Y - X\Theta)^T (Y - X\Theta)) \\
    &= \frac{1}{2} \Tr((Y^T Y - Y^TX\Theta - \Theta^TX^TY + \Theta^TX^TX\Theta))\\
    &= \frac{1}{2} \Tr(Y^T Y) - \Tr(Y^T X\Theta) + \frac{1}{2} \Tr(\Theta^TX^TX\Theta)
\end{align*}

Now we can take the gradient:
\begin{align*}
    \nabla_{\Theta} J(\Theta) &= \nabla_{\Theta} \left( \frac{1}{2} \Tr(Y^T Y) - \Tr(Y^T X\Theta) + \frac{1}{2} \Tr(\Theta^TX^TX\Theta) \right)\\
    &= \nabla_{\Theta} \left( \frac{1}{2} \Tr(Y^T Y) \right) - \nabla_{\Theta} \left( \Tr(Y^T X\Theta) \right) + \nabla_{\Theta} \left( \frac{1}{2} \Tr(\Theta^TX^TX\Theta) \right)\\
    &= 0 - X^TY + X^T X \Theta\\
    &= X^T X \Theta - X^T Y
\end{align*}

Now, we set the gradient to 0 to find the minimum:
\begin{align*}
    X^T X \Theta &= X^T Y\\
    \Theta &= (X^T X)^{-1} X^T Y
\end{align*}

\newpage

\subsection{Component-wise Outputs}

Say we have $y_j \in \mathbb{R}$ for $j = 1, 2, \dots, p$. We thus now are considering 1 dimension of the $p$ dimensions of the output. We want to see is that if we do $p$ different models, which each take a vector of $n$ parameters each (instead of $n\times p$ parameters), we want to see if we get the same solution.

$x \in \mathbb{R}^n$. Let's solve for this. Say $\Theta_j$ is the parameter vector for the $j$th component. Then, our predicted value for the $j$th component is:
\begin{equation*}
    \hat{y}_j = \theta_j^T x
\end{equation*}

Now, we can write the loss function as:
\begin{equation*}
    J(\Theta) = \frac{1}{2} \sum_{i=1}^{n} (y_j - \theta_j^T x^{(i)})^2
\end{equation*}

This is just the sum of squares of each term. So (I am omitting the derivation as we have done this in class multiple times, and a higher order of this in the previous question), we get:
\begin{equation*}
    J(\Theta_j) = \frac{1}{2} (X\Theta_j - Y_j)^T(X\Theta_j - Y_j)
\end{equation*}

where:
\begin{equation*}
    \Theta_j = \begin{bmatrix}
        \theta_{j1} \\
        \theta_{j2} \\
        \vdots \\
        \theta_{jn}
    \end{bmatrix}\  Y_j = \begin{bmatrix}
        y_j^{(1)} \\
        y_j^{(2)} \\
        \vdots \\
        y_j^{(n)}
    \end{bmatrix}
\end{equation*}

We can then take the gradient of this with respect to $\Theta_j$ to minimise the loss function:
\begin{align*}
    &= \nabla_{\Theta_j} \left( \frac{1}{2} (X\Theta_j - Y_j)^T(X\Theta_j - Y_j) \right)\\
    &= \frac{1}{2} \nabla_{\Theta_j} \left( (X\Theta_j)^TX\Theta_j - (X\Theta_j)^TY_j - Y_j^T(X\Theta_j) + Y_j^TY_j \right)\\
    &= \frac{1}{2} \left( \Theta_j^T(X^TX)\Theta_j - 2(X^TY_j)^T\Theta_j \right)\\
    &= X^TX\Theta_j - X^TY_j
\end{align*}

Setting this to 0, we get:
\begin{equation*}
    X^TX\Theta_j = X^TY_j
\end{equation*}

Finally, from this $\Theta_j = (X^TX)^{-1}X^TY_j$.

We will have $p$ such $\Theta_j$s, one for each component:
\begin{equation*}
    \Theta = \begin{bmatrix}
        \Theta_1 \\
        \Theta_2 \\
        \vdots \\
        \Theta_p
    \end{bmatrix}
\end{equation*}

This is precisely the same as what we had in the previous question. So we get the same solution.

\end{document}